# Importing necessary packages
import transformers>=4.31.0  # For transformer models
import datasets~=2.3.2  # For handling datasets
import rouge_score~=0.1.2  # For evaluating summaries
import sentencepiece~=0.1.99  # For subword tokenization
from typing-extensions import TypedDict  # For type hinting
from tqdm import tqdm  # For progress bars

# Defining a custom class for summary evaluation
class SummaryEvaluation(TypedDict):
    rouge1: float
    rouge2: float
    rougeL: float

# Loading the pre-trained model and tokenizer
model_name = "t5-base"
model = transformers.T5ForConditionalGeneration.from_pretrained(model_name)
tokenizer = transformers.T5Tokenizer.from_pretrained(model_name)

# Loading the dataset
dataset = datasets.load_dataset("cnn_dailymail", "3.0.0")

# Preprocessing the dataset
def preprocess_function(examples):
    inputs = tokenizer(examples["text"], truncation=True, padding="max_length")
    return {"input_ids": inputs["input_ids"], "attention_mask": inputs["attention_mask"]}

preprocessed_dataset = dataset.map(preprocess_function, batched=True)

# Splitting the dataset into train and validation sets
train_size = int(0.8 * len(preprocessed_dataset))
val_size = len(preprocessed_dataset) - train_size
train_dataset = preprocessed_dataset.train_test_split(train_size, shuffle=True)
val_dataset = preprocessed_dataset.train_test_split(val_size, shuffle=True)

# Training the model
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=
